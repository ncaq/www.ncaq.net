---
title: mcp-nixosのHTTPエンドポイントをmicrovm.nixで建ててパブリックに公開した
---

# 背景

[mcp-nixos](https://github.com/utensils/mcp-nixos)は、
NixOSのパッケージやオプション、
Home Manager、
nix-darwinなどの情報をMCP(Model Context Protocol)経由で提供するサーバです。
AIアシスタントがNixOS関連のパッケージ名や設定をハルシネーションすることを防いでくれます。

普段はPC上のClaude Codeからstdioモードで使っていたのですが、
AndroidのClaude(claude.ai)からも使いたくなりました。
claude.aiのRemote MCP機能を使えば、
HTTPエンドポイントを持つMCPサーバに接続できます。

# なぜパブリックにしたのか

最初はTailscaleなどの認証付きネットワーク経由で公開しようと考えていました。
しかしclaude.aiのRemote MCPは、
Claudeのサーバからエンドポイントにアクセスする仕組みです。
つまり自分のマシンではなくAnthropicのサーバからリクエストが飛んでくるので、
Tailscaleのようなデバイス認証は使えませんでした。

OAuthを使う方法もありますが、
使うのが自分一人だとしてもOAuthの実装はかなり面倒くさいことが分かりました。
そもそもmcp-nixosは読み取り専用のMCPサーバなので、
認証をつける必然性もあまり感じません。
面倒くささに見合わないのでやめました。

どうせパブリックにするなら、
NixOSコミュニティへの貢献にもなります。
自分以外の人もclaude.aiなどからmcp-nixosを使えるようになるので。

リスクについては、
仮想マシンで隔離すれば万が一脆弱性を突かれても被害を最小限に抑えられると判断しました。
現状でもCloudflare TunnelやTailscaleで自宅サーバのサービスをインターネットに公開しているので、
許容範囲です。

# なぜmicrovm.nixか

[microvm.nix](https://github.com/microvm-nix/microvm.nix)は、
NixOS Flakeを使って軽量なNixOS仮想マシンを構築・実行するプロジェクトです。
[QEMU](https://www.qemu.org/)、
[cloud-hypervisor](https://www.cloudhypervisor.org/)、
[Firecracker](https://firecracker-microvm.github.io/)(AWS Lambdaの基盤として使われている)
など複数のハイパーバイザーに対応しています。

以前、
[NixOS containersでForgejoとAtticdをコンテナ化した - ncaq](https://www.ncaq.net/2026/01/10/19/47/10/)
に書いたようにNixOS containersでサービスを分離していました。
NixOS containersはsystemd-nspawnベースのコンテナで、
カーネルをホストと共有します。

今回はインターネットに直接公開するので、
仮想マシンの方が安心です。
コンテナだと共有カーネルの攻撃対象領域が広いですが、
仮想マシンならカーネルが分離されるうえに、
ハードウェアも仮想化されるため攻撃対象領域がハイパーバイザーとそのデバイスドライバーに限定されます。

microvm.nixを選んだ理由は、
NixOS containersと同じように宣言的に書けるからです。
実際使ってみたところ、
コンテナとほぼ同じ感覚で設定を書けました。

# mcp-nixosのHTTPモード起動方法の調査

mcp-nixosは[FastMCP](https://gofastmcp.com/)フレームワークを使用しています。
FastMCP 2.xは複数のトランスポートモードをサポートしているため、
コード変更なしでHTTPモードで起動できます。

```bash
fastmcp run mcp_nixos.server:mcp --transport streamable-http --host 0.0.0.0 --port 8080
```

`streamable-http`が現在推奨されているトランスポートです。
SSEもサポートされていますが後方互換性のためのもので、
新規では使わない方が良いでしょう。

# 実装

[feat: mcp-nixosのHTTPエンドポイントを建てる by ncaq · Pull Request #582 · ncaq/dotfiles](https://github.com/ncaq/dotfiles/pull/582)
で実装しました。

## コンテナとmicroVMのアドレス管理を統一

既存の`container-mapping.nix`を`machine-mapping.nix`にリネームして、
コンテナとmicroVMの両方を統一的に管理するようにしました。

```nix
options.machineAddresses = lib.mkOption {
  type = lib.types.attrsOf addressType;
  default = {
    forgejo = {
      host = "192.168.100.10";
      guest = "192.168.100.11";
    };
    atticd = {
      host = "192.168.100.20";
      guest = "192.168.100.21";
    };
    mcp-nixos = {
      host = "192.168.100.30";
      guest = "192.168.100.31";
    };
  };
  description = "Machine (container/microVM) network addresses";
};
```

`containerAddresses.*.container`を`machineAddresses.*.guest`に変更しました。
`guest`という名前ならコンテナでもmicroVMでも意味が通じます。

NATの内部インターフェースにも`vm-+`(microVMのTAPインターフェース)を追加しています。

```nix
networking.nat = {
  enable = true;
  internalInterfaces = [
    "ve-+" # container veth interfaces
    "vm-+" # microVM TAP interfaces
  ];
};
```

## vsock CIDの一元管理

[refactor: microVMのvsock CID管理をmachine-mappingに一元化 by ncaq · Pull Request #600 · ncaq/dotfiles](https://github.com/ncaq/dotfiles/pull/600)
で、
vsock CIDの割り当てもmachine-mapping.nixで管理するようにしました。

```nix
options.microvmCid = lib.mkOption {
  type = lib.types.attrsOf (lib.types.ints.between 3 4294967294);
  default = {
    mcp-nixos = 3;
  };
  description = "vsock CID assignments for microVMs (must be >= 3, unique per VM)";
};
```

vsock CIDは仮想マシンを識別する32bit整数値です。
0はハイパーバイザー、
1はループバック、
2はホストに予約されているため、
3以上を使います。

cloud-hypervisorはvsock経由でsystemd-notifyが使えるので、
ホストのsystemdがVM内のサービス起動完了を正確に検知できます。

重複チェックのアサーションも入れてあります。
microVMが増えてもCIDの衝突を防ぎやすくなります。

## mcp-nixos.nixの設定

### microVM設定

```nix
{ pkgs, config, ... }:
let
  addr = config.machineAddresses.mcp-nixos;
  # Pythonの依存関係とパッケージ自体をまとめて環境にします。
  # mcp-nixosコマンドを直接実行するわけではないので依存パッケージの別途指定が必要です。
  mcp-nixos-env = pkgs.python3.withPackages (
    _: pkgs.mcp-nixos.propagatedBuildInputs ++ [ pkgs.mcp-nixos ]
  );
  # HTTPで提供するためにmcp-nixosをコマンドラインで動かすのではなくPythonモジュールを呼び出します。
  serverPy = "${pkgs.mcp-nixos}/${pkgs.python3.sitePackages}/mcp_nixos/server.py";
in
{
  microvm.vms.mcp-nixos = {
    inherit pkgs;
    config = {
      microvm = {
        hypervisor = "cloud-hypervisor";
        vcpu = 1;
        mem = 768;
        vsock.cid = config.microvmCid.mcp-nixos;
        interfaces = [
          {
            type = "tap";
            id = "vm-mcp-nixos";
            mac = "02:00:00:00:00:30";
          }
        ];
        shares = [
          {
            tag = "ro-store";
            source = "/nix/store";
            mountPoint = "/nix/.ro-store";
            proto = "virtiofs";
          }
        ];
      };
      # ...
    };
  };
}
```

### ポイント

#### 依存関係の解決

ここで少し苦労しました。
mcp-nixosはstdioモードで動かす前提のパッケージなので、
`fastmcp run`でHTTPサーバとして動かすには依存関係を自分で解決する必要があります。

`pkgs.python3.withPackages`でmcp-nixosの`propagatedBuildInputs`とmcp-nixos自体をまとめたPython環境を作り、
その環境の`fastmcp`コマンドを使うことで解決しました。

```nix
mcp-nixos-env = pkgs.python3.withPackages (
  _: pkgs.mcp-nixos.propagatedBuildInputs ++ [ pkgs.mcp-nixos ]
);
```

#### cloud-hypervisorの選択

ハイパーバイザーにはcloud-hypervisorを使いました。
Rustで書かれていて、
vsock対応でsystemd-notify連携もできます。

#### リソース割り当て

vCPUは1、メモリは768MBです。
mcp-nixosはPythonのHTTPサーバなのでこの程度で十分です。

#### /nix/storeの共有

ホストの`/nix/store`をvirtiofs経由で読み取り専用マウントしています。
これにより仮想マシン側でNixのパッケージを利用できます。

### HTTPサーバのsystemdサービス

```nix
systemd.services.mcp-nixos-http = {
  description = "mcp-nixos HTTP server";
  after = [ "network.target" ];
  wantedBy = [ "multi-user.target" ];
  serviceConfig = {
    ExecStart = "${mcp-nixos-env}/bin/fastmcp run ${serverPy}:mcp --transport streamable-http --host 0.0.0.0 --port 8080";
    DynamicUser = true;
    Restart = "always";
    RestartSec = 5;
    NoNewPrivileges = true;
    ProtectSystem = "strict";
    ProtectHome = true;
    PrivateTmp = true;
    PrivateDevices = true;
  };
};
```

仮想マシンの中でさらにsystemdのハードニングオプションを適用しています。
DynamicUserで動的にユーザーを生成し、
ProtectSystemやProtectHomeでファイルシステムへのアクセスを制限しています。

`ExecStart`では`mcp-nixos-env`で作ったPython環境の`fastmcp`を使っています。
`lib.getExe pkgs.python3Packages.fastmcp`のように単独で呼び出すとmcp-nixosの依存関係が解決されないので動きません。

### 帯域制限

帯域制限はmicroVMの外側、
ホストのTAPインターフェースに対して設定しています。

```nix
systemd.services.mcp-nixos-traffic-control = {
  description = "Traffic control for mcp-nixos microVM";
  requires = [ "microvm-tap-interfaces@mcp-nixos.service" ];
  after = [ "microvm-tap-interfaces@mcp-nixos.service" ];
  bindsTo = [ "microvm-tap-interfaces@mcp-nixos.service" ];
  wantedBy = [ "microvm-tap-interfaces@mcp-nixos.service" ];
  serviceConfig = {
    Type = "oneshot";
    RemainAfterExit = true;
    ExecStart = "${pkgs.iproute2}/bin/tc qdisc replace dev vm-mcp-nixos root tbf rate 100mbit burst 10mbit latency 400ms";
  };
};
```

万が一脆弱性を突かれて踏み台にされた場合に、
外部への攻撃トラフィックで迷惑をかけないよう帯域を100mbitに制限しています。
`tc qdisc replace`を使うことで冪等性を確保しています。
TAPインターフェースのライフサイクルに`bindsTo`で紐づけることで、
microVMの停止時にも適切にクリーンアップされます。

## Cloudflare Tunnelの設定

```nix
"mcp-nixos.ncaq.net" = "http://${config.machineAddresses.mcp-nixos.guest}:8080";
```

Cloudflare Tunnel経由で`https://mcp-nixos.ncaq.net/mcp/`としてアクセスできるようにしました。

# 後から修正した部分

## コンテナvethインターフェースの不要なfirewall信頼設定を削除

[fix: コンテナvethインターフェースの不要なfirewall信頼設定を削除 by ncaq · Pull Request #601 · ncaq/dotfiles](https://github.com/ncaq/dotfiles/pull/601)
で、
`networking.firewall.trustedInterfaces`から`ve-+`を削除しました。

ホストからコンテナへの通信はホスト側が接続を開始するため、
conntrackによりESTABLISHEDとして自動許可されます。
PostgreSQLもUnixソケットのbind mount経由なのでネットワークを使いません。
microVMのTAPインターフェース(`vm-+`)も信頼設定なしで動作していたので、
コンテナ側も同様に不要でした。

PR #582のレビューで「microVMを隔離する設計意図と矛盾する」と指摘されたのがきっかけです。

## vsock CIDの設定と一元化

[feat(microvm): microvmのvsock.cidを3に設定 by ncaq · Pull Request #586 · ncaq/dotfiles](https://github.com/ncaq/dotfiles/pull/586)
でvsock CIDを設定し、
その後の[PR #600](https://github.com/ncaq/dotfiles/pull/600)で管理をmachine-mapping.nixに一元化しました。

# 感想

microvm.nixを使ってみて、
NixOS containersに比べても別に全然面倒くさくなく宣言的に書けると感じました。
`microvm.vms.<name>.config`の中にNixOSの設定を書くだけで、
コンテナの`containers.<name>.config`と同じ感覚です。

違いとしてはファイル共有がbind mountではなくvirtiofs/9pになることと、
固定メモリ割り当てが必要なことぐらいです。
パフォーマンスがそこまで気にならない用途であれば、
セキュリティを重視したいときに気軽に使えそうです。

今回のようにインターネットに直接公開するサービスは仮想マシンで隔離して、
内部向けのサービスはコンテナで軽く動かすという使い分けが良いのかなと思います。

# まとめ

mcp-nixosのHTTPエンドポイントをmicrovm.nixで建ててパブリックに公開しました。

- AndroidのClaude(claude.ai)からNixOS情報にアクセスするのが目的
- 読み取り専用MCPに認証は不要と判断してパブリック公開
- microvm.nixでcloud-hypervisorベースの仮想マシンに隔離
- 帯域制限やsystemdハードニングでDoS対策
- Cloudflare Tunnel経由で`https://mcp-nixos.ncaq.net/mcp/`として公開
- 既存のコンテナ管理と統一するためmachine-mapping.nixに一元化

主なソースコードは[ncaq/dotfiles](https://github.com/ncaq/dotfiles)にあります。
